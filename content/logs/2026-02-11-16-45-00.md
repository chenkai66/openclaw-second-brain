---
date: 2026-02-11-16-45-00
type: daily-log
summary: "Exploring Docker best practices and container optimization"
topics: ["docker", "devops", "containers"]
ai_generated: true
---

# 2026-02-11-16-45-00

## Conversation: Docker Optimization Techniques

### Initial Question
**User:** My Docker images are huge (over 1GB). How can I reduce the size?

**AI:** Image size is a common issue! Here are the most effective strategies:

**1. Use Alpine base images:**
```dockerfile
# Before: ~900MB
FROM node:18

# After: ~170MB
FROM node:18-alpine
```

**2. Multi-stage builds:**
```dockerfile
# Build stage
FROM node:18 AS builder
WORKDIR /app
COPY package*.json ./
RUN npm ci
COPY . .
RUN npm run build

# Production stage - only copy what's needed
FROM node:18-alpine
WORKDIR /app
COPY package*.json ./
RUN npm ci --only=production
COPY --from=builder /app/dist ./dist
CMD ["node", "dist/server.js"]
```

This can reduce your image from 1GB to 200MB!

**3. Minimize layers:**
```dockerfile
# Bad: 3 layers
RUN apt-get update
RUN apt-get install -y curl
RUN apt-get install -y git

# Good: 1 layer
RUN apt-get update && \
    apt-get install -y curl git && \
    rm -rf /var/lib/apt/lists/*
```

### Follow-up Question
**User:** What about build time? My builds take forever!

**AI:** Build speed is crucial for developer productivity. Here's how to optimize:

**1. Leverage build cache - order matters:**
```dockerfile
# Copy package files first (changes rarely)
COPY package*.json ./
RUN npm ci

# Copy source code last (changes frequently)
COPY . .
RUN npm run build
```

When you change source code, npm dependencies don't reinstall!

**2. Use .dockerignore:**
```
node_modules
.git
.env
*.md
.vscode
coverage
dist
```

This prevents copying unnecessary files, speeding up context transfer.

**3. Use BuildKit:**
```bash
# Enable BuildKit
export DOCKER_BUILDKIT=1
docker build -t myapp .

# Or in docker-compose.yml
export COMPOSE_DOCKER_CLI_BUILD=1
export DOCKER_BUILDKIT=1
```

BuildKit provides parallel builds and better caching.

### Security Question
**User:** How do I make my containers more secure?

**AI:** Security is critical! Here are essential practices:

**1. Don't run as root:**
```dockerfile
# Create non-root user
RUN addgroup -g 1001 -S nodejs && \
    adduser -S nodejs -u 1001

# Change ownership
RUN chown -R nodejs:nodejs /app

# Switch to non-root user
USER nodejs
```

**2. Use specific image tags:**
```dockerfile
# Bad: tag can change
FROM node:latest

# Good: specific version
FROM node:18.16.0-alpine
```

**3. Scan for vulnerabilities:**
```bash
# Using Docker Scout
docker scout cves myapp:latest

# Using Trivy
trivy image myapp:latest
```

**4. Don't store secrets in images:**
```dockerfile
# Bad: secret in image
ENV API_KEY=secret123

# Good: pass at runtime
docker run -e API_KEY=$API_KEY myapp
```

**5. Use read-only filesystem:**
```bash
docker run --read-only --tmpfs /tmp myapp
```

### Networking Question
**User:** How do containers communicate with each other?

**AI:** Great question! Docker networking is powerful:

**1. Default bridge network:**
```bash
# Containers can't use service names
docker run -d --name web nginx
docker run -d --name api node-app

# api can't reach web by name
```

**2. Custom bridge network (recommended):**
```bash
# Create network
docker network create mynetwork

# Run containers on network
docker run -d --network mynetwork --name web nginx
docker run -d --network mynetwork --name api node-app

# Now api can reach web at http://web:80
```

**3. Docker Compose (automatic networking):**
```yaml
version: '3.8'
services:
  web:
    image: nginx
  api:
    image: node-app
    environment:
      - WEB_URL=http://web:80
```

Compose creates a network automatically, and services can reach each other by name!

### Practical Example
**User:** Can you show me a complete production-ready setup?

**AI:** Absolutely! Here's a full-stack application:

**docker-compose.yml:**
```yaml
version: '3.8'

services:
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile.prod
    ports:
      - "3000:80"
    depends_on:
      - backend
    restart: unless-stopped

  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
    environment:
      - NODE_ENV=production
      - DATABASE_URL=postgresql://user:pass@db:5432/mydb
      - REDIS_URL=redis://redis:6379
    depends_on:
      db:
        condition: service_healthy
      redis:
        condition: service_started
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 512M

  db:
    image: postgres:15-alpine
    environment:
      POSTGRES_USER: user
      POSTGRES_PASSWORD: pass
      POSTGRES_DB: mydb
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U user"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    command: redis-server --appendonly yes
    volumes:
      - redis_data:/data
    restart: unless-stopped

  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
      - ./ssl:/etc/nginx/ssl:ro
    depends_on:
      - frontend
      - backend
    restart: unless-stopped

volumes:
  postgres_data:
  redis_data:
```

**Backend Dockerfile:**
```dockerfile
FROM node:18-alpine AS builder

WORKDIR /app
COPY package*.json ./
RUN npm ci
COPY . .
RUN npm run build

FROM node:18-alpine

RUN addgroup -g 1001 -S nodejs && \
    adduser -S nodejs -u 1001

WORKDIR /app

COPY package*.json ./
RUN npm ci --only=production && \
    npm cache clean --force

COPY --from=builder --chown=nodejs:nodejs /app/dist ./dist

USER nodejs

EXPOSE 8000

HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
  CMD node -e "require('http').get('http://localhost:8000/health', (r) => {process.exit(r.statusCode === 200 ? 0 : 1)})"

CMD ["node", "dist/server.js"]
```

**Frontend Dockerfile.prod:**
```dockerfile
FROM node:18-alpine AS builder

WORKDIR /app
COPY package*.json ./
RUN npm ci
COPY . .
RUN npm run build

FROM nginx:alpine

COPY --from=builder /app/build /usr/share/nginx/html
COPY nginx.conf /etc/nginx/conf.d/default.conf

EXPOSE 80

CMD ["nginx", "-g", "daemon off;"]
```

### Monitoring Question
**User:** How do I monitor my containers in production?

**AI:** Monitoring is essential! Here are the tools:

**1. Built-in Docker stats:**
```bash
# Real-time stats
docker stats

# Specific container
docker stats backend
```

**2. Health checks:**
```dockerfile
HEALTHCHECK --interval=30s --timeout=3s \
  CMD curl -f http://localhost:8000/health || exit 1
```

**3. Logging:**
```bash
# View logs
docker logs backend

# Follow logs
docker logs -f backend

# Last 100 lines
docker logs --tail 100 backend
```

**4. Prometheus + Grafana:**
```yaml
services:
  prometheus:
    image: prom/prometheus
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
    ports:
      - "9090:9090"

  grafana:
    image: grafana/grafana
    ports:
      - "3001:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
```

### Key Takeaways

1. Use Alpine images to reduce size (900MB â†’ 170MB)
2. Multi-stage builds separate build and runtime
3. Order Dockerfile commands for better caching
4. Always run containers as non-root user
5. Use custom networks for service discovery
6. Implement health checks for reliability
7. Set resource limits to prevent resource exhaustion
8. Use docker-compose for multi-container apps

### Related Topics
- Kubernetes for orchestration
- Docker Swarm for clustering
- CI/CD with Docker
- Container security scanning

> This log was automatically generated from a conversation session.
